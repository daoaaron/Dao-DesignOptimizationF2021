{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f72d45d-bd7e-44ba-abce-6348bc7ad97d",
   "metadata": {},
   "source": [
    "# Problem 1 (50 points) \n",
    "\n",
    "Vapor-liquid equilibria data are correlated using two adjustable parameters $A_{12}$ and $A_{21}$ per binary\n",
    "mixture. For low pressures, the equilibrium relation can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p = & x_1\\exp\\left(A_{12}\\left(\\frac{A_{21}x_2}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{water}^{sat}\\\\\n",
    "& + x_2\\exp\\left(A_{21}\\left(\\frac{A_{12}x_1}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{1,4 dioxane}^{sat}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here the saturation pressures are given by the Antoine equation\n",
    "\n",
    "$$\n",
    "\\log_{10}(p^{sat}) = a_1 - \\frac{a_2}{T + a_3},\n",
    "$$\n",
    "\n",
    "where $T = 20$($^{\\circ}{\\rm C}$) and $a_{1,2,3}$ for a water - 1,4 dioxane\n",
    "system is given below.\n",
    "\n",
    "|             | $a_1$     | $a_2$      | $a_3$     |\n",
    "|:------------|:--------|:---------|:--------|\n",
    "| Water       | 8.07131 | 1730.63  | 233.426 |\n",
    "| 1,4 dioxane | 7.43155 | 1554.679 | 240.337 |\n",
    "\n",
    "\n",
    "The following table lists the measured data. Recall that in a binary system $x_1 + x_2 = 1$.\n",
    "\n",
    "|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n",
    "|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
    "|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n",
    "\n",
    "Estimate $A_{12}$ and $A_{21}$ using data from the above table: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bac918-36cd-411f-8c17-d3e2a3123e7b",
   "metadata": {},
   "source": [
    "## Part 1.\n",
    "**Formulate the least square problem.**\n",
    "$$\\min_{A_{12}, A_{21}} \\sum_{i=1}^{n} (p(x^{(i)}, A_{12},A_{21})-p^{(i)})^2  \\quad \\forall i=1,2,...11$$\n",
    "\n",
    "such that  $$p(x^{(i)}, A_{12},A_{21})=x^{(i)}_1\\exp\\left(A_{12}\\left(\\frac{A_{21}x^{(i)}_2}{A_{12}x^{(i)}_1+A_{21}x^{(i)}_2}\\right)^2\\right)p_{water}^{sat} + x_2\\exp\\left(A_{21}\\left(\\frac{A_{12}x^{(i)}_1}{A_{12}x^{(i)}_1+A_{21}x^{(i)}_2}\\right)^2\\right)p_{1,4 dioxane}^{sat} $$\n",
    "and $$ x_2 = 1-x_1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d37305-f55a-43e7-b4a2-0256456797b7",
   "metadata": {},
   "source": [
    "## Part 2. \n",
    "**Since the model is nonlinear, the problem does not have an analytical solution. Therefore, solve it using the gradient descent or Newton's method implemented in HW1.** <br>\n",
    "We will calculate the saturation pressures, using $p^{sat}=10^{a_1-\\frac{a_2}{T+a_3}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93ce9ddc-ebde-40ee-840c-79b97d23cabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.460784103526855\n",
      "28.824099527405245\n"
     ]
    }
   ],
   "source": [
    "# HOUSEKEEPIN'\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "p_satw=10**(8.071 - 1730.63/(20+233.426)) # 17.460784103526855 \n",
    "p_sat14=10**(7.43155 - 1554.679/(20+240.337)) # 28.824099527405245   \n",
    "x=[ 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , 1.0 ]\n",
    "p=[ 28.1 , 34.4 , 36.7 , 36.9 , 36.8 , 36.7 , 36.5 , 35.4 , 32.9 , 27.7 , 17.5 ]\n",
    "\n",
    "print(p_satw)\n",
    "print(p_sat14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5227c318-6d2d-4ce3-a372-249e83b22041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final a is now [1.9594922 1.6899538]\n",
      "The objective function is now 0.6712008\n"
     ]
    }
   ],
   "source": [
    "# The FUNCTIONS.\n",
    "\n",
    "def press(a,xi): # Calculating pressure!!\n",
    "    x1=xi\n",
    "    x2=1-x1\n",
    "    A12= a[0]\n",
    "    A21= a[1]\n",
    "    return x1 * t.exp(A12* ( (A21*x2)/(A12*x1 + A21*x2) )**2 ) * p_satw + x2 * t.exp(A21* ( (A12*x1)/(A12*x1 + A21*x2) )**2 ) * p_sat14\n",
    "\n",
    "def objf(a): # Total sum objective function...\n",
    "    total=0\n",
    "    for i in range(len(x)):\n",
    "        xi=x[i]\n",
    "        pres=press(a,xi)\n",
    "        #print(press)\n",
    "        total = total + (pres-p[i])**2  # Sum the squared difference.\n",
    "    return total\n",
    "\n",
    "def lines(a):  # because you just gotta have a line search!\n",
    "    step=.1 # initiate\n",
    "    while objf(a-step*a.grad) > objf(a)-step*(0)*np.matmul(a.grad,a.grad):\n",
    "        step=.5*step\n",
    "    return step\n",
    "\n",
    "# VARIABLE SETUP.\n",
    "\n",
    "a = Variable(t.tensor([1.0, 6]), requires_grad=True) \n",
    "e = 100 # Also our error term! Initialize.\n",
    "\n",
    "# RUN THIS THING!\n",
    "while e > 0.1:  # Error stop criteria. \n",
    "    objective=objf(a)\n",
    "    objective.backward()\n",
    "    step=lines(a)\n",
    "    #print('Objective func is now ' +str(objective.data.numpy()))\n",
    "    e = t.linalg.norm(a.grad) # update the error value.\n",
    "    with t.no_grad():\n",
    "        #print('a is ' + str(a))\n",
    "        #print('grad is ' + str(a.grad))\n",
    "        a -= step * a.grad\n",
    "        #print('a is now ' + str(a) + ' and e is '+str(e))\n",
    "        #print('step')\n",
    "        # need to clear the gradient at every step, or otherwise it will accumulate...\n",
    "        a.grad.zero_()\n",
    "        \n",
    "print('Final a is now ' + str(a.data.numpy()))\n",
    "print('The objective function is now ' + str(objective.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd285734-6fda-4519-9506-30913b4e4b94",
   "metadata": {},
   "source": [
    "## Part 3. \n",
    "**Compare your optimized model with the data. Does your model fit well with the data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "632faebf-e73f-41d7-9996-01f237daa1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Pressures   Model Pressures\n",
      "28.1             28.824098587036133\n",
      "34.4             34.64545822143555\n",
      "36.7             36.45291519165039\n",
      "36.9             36.86630630493164\n",
      "36.8             36.87278366088867\n",
      "36.7             36.749027252197266\n",
      "36.5             36.39037322998047\n",
      "35.4             35.3852653503418\n",
      "32.9             32.9476203918457\n",
      "27.7             27.72649383544922\n",
      "17.5             17.460784912109375\n"
     ]
    }
   ],
   "source": [
    "# Well, time to calculate these new p values.\n",
    "\n",
    "a_solve=a.data\n",
    "\n",
    "print('Data Pressures   Model Pressures')\n",
    "for i in range(len(x)):\n",
    "    print(str(p[i]) + '             ' + str(press(a_solve,x[i]).item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1377f9-81cd-4ab8-a339-2029cc8ecb27",
   "metadata": {},
   "source": [
    "The model-generated pressures fit very well ($\\pm 0.75$) with the values in the original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d96224-cf12-415b-825d-1f44f122d1ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Problem 2 (50 points) \n",
    "\n",
    "Solve the following problem using Bayesian Optimization:\n",
    "$$\n",
    "    \\min_{x_1, x_2} \\quad \\left(4-2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2,\n",
    "$$\n",
    "for $x_1 \\in [-3,3]$ and $x_2 \\in [-2,2]$. A tutorial on Bayesian Optimization can be found [here](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "580f4312-41d1-4a30-847f-b896b56e624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## USING THE TUTORIAL CODE...\n",
    "\n",
    "# Housekeeping. \n",
    "\n",
    "import numpy as np\n",
    "import sklearn.gaussian_process as gp\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# EXPECTED IMPROVEMENT function.\n",
    "\n",
    "def expected_improvement(x, gaussian_process, evaluated_loss, greater_is_better=False, n_params=1):\n",
    "    \n",
    "    x_to_predict = x.reshape(-1, n_params)\n",
    "\n",
    "    mu, sigma = gaussian_process.predict(x_to_predict, return_std=True)\n",
    "\n",
    "    if greater_is_better:\n",
    "        loss_optimum = np.max(evaluated_loss)\n",
    "    else:\n",
    "        loss_optimum = np.min(evaluated_loss)\n",
    "\n",
    "    scaling_factor = (-1) ** (not greater_is_better)\n",
    "\n",
    "    # In case sigma equals zero\n",
    "    with np.errstate(divide='ignore'):\n",
    "        Z = scaling_factor * (mu - loss_optimum) / sigma\n",
    "        expected_improvement = scaling_factor * (mu - loss_optimum) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        expected_improvement[sigma == 0.0] == 0.0\n",
    "\n",
    "    return -1 * expected_improvement\n",
    "\n",
    "# SAMPLE NEXT HYPERPARAMETER function.\n",
    "\n",
    "def sample_next_hyperparameter(acquisition_func, gaussian_process, evaluated_loss, greater_is_better=False,\n",
    "                               bounds=(0, 10), n_restarts=25):\n",
    "    best_x = None\n",
    "    best_acquisition_value = 1\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    for starting_point in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, n_params)):\n",
    "\n",
    "        res = minimize(fun=acquisition_func,\n",
    "                       x0=starting_point.reshape(1, -1),\n",
    "                       bounds=bounds,\n",
    "                       method='L-BFGS-B',\n",
    "                       args=(gaussian_process, evaluated_loss, greater_is_better, n_params))\n",
    "\n",
    "        if res.fun < best_acquisition_value:\n",
    "            best_acquisition_value = res.fun\n",
    "            best_x = res.x\n",
    "\n",
    "    return best_x\n",
    "     \n",
    "\n",
    "#%% And then finally, the BAYESIAN OPTIMIZATION function.\n",
    "\n",
    "def bayesian_optimisation(n_iters, sample_loss, bounds, x0=None, n_pre_samples=5,\n",
    "                          gp_params=None, random_search=False, alpha=1e-5, epsilon=1e-7):\n",
    "   \n",
    "    x_list = []\n",
    "    y_list = []\n",
    "\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    if x0 is None:\n",
    "        for params in np.random.uniform(bounds[:, 0], bounds[:, 1], (n_pre_samples, bounds.shape[0])):\n",
    "            x_list.append(params)\n",
    "            y_list.append(sample_loss(params))\n",
    "    else:\n",
    "        for params in x0:\n",
    "            x_list.append(params)\n",
    "            y_list.append(sample_loss(params))\n",
    "\n",
    "    xp = np.array(x_list)\n",
    "    yp = np.array(y_list)\n",
    "\n",
    "    # Create the GP\n",
    "    if gp_params is not None:\n",
    "        model = gp.GaussianProcessRegressor(**gp_params)\n",
    "    else:\n",
    "        kernel = gp.kernels.Matern()\n",
    "        model = gp.GaussianProcessRegressor(kernel=kernel,\n",
    "                                            alpha=alpha,\n",
    "                                            n_restarts_optimizer=10,\n",
    "                                            normalize_y=True)\n",
    "\n",
    "    for n in range(n_iters):\n",
    "\n",
    "        model.fit(xp, yp)\n",
    "\n",
    "        # Sample next hyperparameter\n",
    "        if random_search:\n",
    "            x_random = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(random_search, n_params))\n",
    "            ei = -1 * expected_improvement(x_random, model, yp, greater_is_better=True, n_params=n_params)\n",
    "            next_sample = x_random[np.argmax(ei), :]\n",
    "        else:\n",
    "            next_sample = sample_next_hyperparameter(expected_improvement, model, yp, greater_is_better=True, bounds=bounds, n_restarts=100)\n",
    "\n",
    "        # Duplicates will break the GP. In case of a duplicate, we will randomly sample a next query point.\n",
    "        if np.any(np.abs(next_sample - xp) <= epsilon):\n",
    "            next_sample = np.random.uniform(bounds[:, 0], bounds[:, 1], bounds.shape[0])\n",
    "\n",
    "        # Sample loss for new set of parameters\n",
    "        cv_score = sample_loss(next_sample)\n",
    "\n",
    "        # Update lists\n",
    "        x_list.append(next_sample)\n",
    "        y_list.append(cv_score)\n",
    "\n",
    "        # Update xp and yp\n",
    "        xp = np.array(x_list)\n",
    "        yp = np.array(y_list)\n",
    "\n",
    "    return xp, yp\n",
    "\n",
    "# Our loss function. AKA OBJECTIVE FUNCTION!!!!\n",
    "def sample_loss(x): # takes in a vector [x1,x2]\n",
    "    x1=x[0]\n",
    "    x2=x[1]\n",
    "    return -1*((4 - 2.1*x1**2 + (x1**4)/3)*x1**2 + x1*x2 + (-4 + 4*(x2**2))*x2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df054a85-e52c-44aa-95c0-e22fbe8d236d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true minimum value of -1.02614400718987 is at [-0.06122449  0.69387755]\n"
     ]
    }
   ],
   "source": [
    "# Start runnin...'\n",
    "x_1 = np.linspace(-3,3)\n",
    "x_2 = np.linspace(-2,2)\n",
    "\n",
    "# We need the cartesian combination of these two vectors\n",
    "param_grid = np.array([[x1i, x2i] for x1i in x_1 for x2i in x_2])\n",
    "\n",
    "real_loss = [sample_loss(params) for params in param_grid]\n",
    "\n",
    "# The maximum is at:\n",
    "print('The true minimum value of ' + str(-np.amax(real_loss)) +' is at '+ str(param_grid[np.array(real_loss).argmax(), :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3d3a6e-dda2-47ce-b7d6-bde0683c71ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running Bayesian Opt.\n",
      "Out of 100 iterations, the minimum value of -1.0314392694751924 is at [ 0.08347011 -0.71028762]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Let's ignore the warnings.\n",
    "\n",
    "bounds = np.array([[-3, 3], [-2, 2]])\n",
    "\n",
    "print('running Bayesian Opt.')\n",
    "\n",
    "xp, yp = bayesian_optimisation(n_iters=100, # HERE WE GO! \n",
    "                               sample_loss=sample_loss, \n",
    "                               bounds=bounds,\n",
    "                               n_pre_samples=3,\n",
    "                               random_search=100000)\n",
    "\n",
    "print('Out of 100 iterations, the minimum value of ' + str(-np.amax(yp)) +' is at ' + str(xp[np.where(yp == np.amax(yp))[0][0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
