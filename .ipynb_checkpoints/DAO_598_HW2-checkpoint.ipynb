{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce950d0",
   "metadata": {},
   "source": [
    "# <div align=\"center\"> MAE 598 Homework 2\n",
    "<div align=\"center\"> Aaron Dao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981aa30c-be5e-46e2-b663-316ac23c9d72",
   "metadata": {},
   "source": [
    "# Theory/Computation Problems\n",
    "\n",
    "### Problem 1 (20 points) \n",
    "**a.** Show that the stationary point (zero gradient) of the function\n",
    "$$\n",
    "    f=2x_{1}^{2} - 4x_1 x_2+ 1.5x^{2}_{2}+ x_2\n",
    "$$\n",
    "is a saddle (with indefinite Hessian). \n",
    "\n",
    "**b.** Find the directions of downslopes away from the saddle. Hint: Use Taylor's expansion at the saddle point. Find directions that reduce $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b738039-a2a0-4646-86e7-a6d1613fc766",
   "metadata": {},
   "source": [
    "#### Solution.\n",
    "##### A.\n",
    "The gradient is $$ g= \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2}  \\end{bmatrix}$$ \n",
    "\n",
    "$$ g= \\begin{bmatrix} 4x_1-4x_2 \\\\ -4x_1+3x_2+1 \\end{bmatrix}$$\n",
    "To find the stationary point, solve: $$\\left[\\begin{array}{cc|c} 4 & -4 & 0 \\\\ -4 & 3 & -1 \\end{array}\\right]$$ which gives $(x_1, x_2)=(1,1)$.\n",
    "\n",
    "The Hessian is \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5854ba62-1600-4a87-8ced-3837f19a07ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4 -4]\n",
      " [-4  3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "H= np.array([[4, -4],[-4, 3]])\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c4fcae-0ef3-4570-9ad2-f87f578b5aae",
   "metadata": {},
   "source": [
    "And the eigenvalues are given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec18a849-7253-4564-9dbc-e82af38c8ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.53112887 -0.53112887]\n"
     ]
    }
   ],
   "source": [
    "w, v = LA.eig(H)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40193b5-cc3b-4350-86b6-1189994f210e",
   "metadata": {},
   "source": [
    "Thus, since $\\lambda_1\\gt0$ and $\\lambda_2\\lt0$, the Hessian is **indefinite**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dbe9bf-3d58-4d1d-85c7-3b5cc91b969a",
   "metadata": {},
   "source": [
    "##### B. \n",
    "The Taylor's Expansion at the saddle is given as $$f(x)= 0.5 + (0) + \\frac{1}{2}\\begin{bmatrix} (x_1-1) & (x_2-1) \\end{bmatrix}\\begin{bmatrix}4 & -4 \\\\\\ -4 & 3\\end{bmatrix}\\begin{bmatrix}(x_1-1) \\\\\\ (x_2-1) \\end{bmatrix}$$ \n",
    "where \n",
    "$$\\begin{bmatrix} (x_1-1) & (x_2-1) \\end{bmatrix}\\begin{bmatrix}4 & -4 \\\\\\ -4 & 3\\end{bmatrix}\\begin{bmatrix}(x_1-1) \\\\\\ (x_2-1) \\end{bmatrix} = 4x_1^2-8x_1x_2+3x_2^2+2x_2-1$$ must be $\\lt0$.\n",
    "In other words, $$4x_1^2-8x_1x_2+3x_2^2+2x_2-1=(2x_1-3x_2+1)(2x_1-x_2-1)\\lt0$$ Thus the directions of downslopes are $$\\bigg\\{ \\begin{bmatrix}x_1 \\\\\\ x_2 \\end{bmatrix} \\forall x_1, x_2: 2x_1-3x_2\\lt-1 \\bigcup 2x_1-x_2\\lt1 \\bigg\\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e507c-3920-41e1-9d29-164b8d651057",
   "metadata": {},
   "source": [
    "### Problem 2 (50 points) \n",
    "\n",
    "**Part 1.** (10 points) Find the point in the plane $x_1+2x_2+3x_3=1$ in $\\mathbb{R}^3$ that is nearest to the point $(-1,0,1)^T$. Is this a convex problem? Hint: Convert the problem into an unconstrained problem using $x_1+2x_2+3x_3=1$.\n",
    "\n",
    "**Solution.** We are looking for a point $x\\in\\mathbb{R}^3$ closest to (or, with the smallest distance to) the point $\\begin{bmatrix}-1 & 0& 1\\end{bmatrix}^T$, which also satisfies the plane definition $\\begin{bmatrix}1&2&3\\end{bmatrix}x=1$. We use the definition of the plane to substitute $1-2x_2-3x_3$ for $x_1$ and receive $$\\min_{x_2,x_3}\\quad(2-2x_2-3x_3)^2+x_2^2+(x_3-1)^2$$ The gradient is $$\\begin{bmatrix} 10x_2+6x_3-8 \\\\ 12x_2 +20x_3-14\\end{bmatrix}$$ which, after solving the system of equations, has a stationary point at $(x_2, x_3)=(-0.1429, 0.7857)$ and thus the full stationary point is $$\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}=\\begin{bmatrix}-1.0714\\\\-0.1429\\\\0.7857\\end{bmatrix}$$ \n",
    "\n",
    "The Hessian at this point is constant, and is $$\\begin{bmatrix} 10 &12\\\\12&10\\end{bmatrix}$$ which is positive definite. Thus the stationary point is a local minimum.\n",
    "\n",
    "As an optimization problem.\n",
    "$$ \\min_{x} \\Bigg\\| x-\\begin{bmatrix}-1\\\\0\\\\1\\end{bmatrix} \\Bigg\\|^2$$ subject to $$\\begin{bmatrix}1&2&3\\end{bmatrix}x=1$$ Yes, this is convex because the constraint is linear, and the objective function is the squared norm of a linear function, all of which (square, norm, and linear operators) are convex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512072fe-e357-49ad-9556-a299c28dd091",
   "metadata": {},
   "source": [
    "**Part 2.** (40 points) Implement the gradient descent and Newton's algorithm for solving the problem. Attach your codes along with a short summary including (1) the initial points tested, (2) corresponding solutions, (3) a log-linear convergence plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a55c448-cbbb-4d76-81f5-005b4b386b9b",
   "metadata": {},
   "source": [
    "#### Gradient Descent.\n",
    "***summary here: initial points, solutions to each point, log-linear convergence plot***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9296991e-fc5b-4dd3-ae7f-b4f67085d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "objfun= lambda x: ((1-2*x[0]-3*x[1])+1)**2 + (x[0])**2 + (x[1]-1)**2\n",
    "grad = lambda x: np.array([10*x[0]+12*x[1]-8, 12*x[0]+20*x[1]-14]) # Note that x2 is x[0] and x3 is x[1]\n",
    "f_true=objfun(np.array([-.1429,.7857]))\n",
    "\n",
    "#%% GRAD SETUP.\n",
    "x0 = np.array([0, 0]).T # Initial guess.\n",
    "x_solve_grad = [x0]\n",
    "x= x_solve_grad[0]\n",
    "a=.01\n",
    "e=10 # Initialize!\n",
    "f_grad=[objfun(x0)]\n",
    "\n",
    "# Let's use an AMIJO line search\n",
    "def amijo(x):\n",
    "    a=1 # initiate\n",
    "    while objfun(x-a*grad(x)) > objfun(x)-a*(.5)*np.matmul(grad(x),grad(x)):\n",
    "        a=.5*a\n",
    "    return a\n",
    "    \n",
    "    \n",
    "while e > .001:\n",
    "    a=amijo(x)\n",
    "    x= x - a*grad(x)\n",
    "    #x_solve=np.concatenate((x_solve, x), axis=1)\n",
    "    x_solve_grad.append(x)\n",
    "    f_grad.append(math.log(abs(objfun(x)-f_true),10))\n",
    "    e = np.linalg.norm(grad(x))\n",
    "    print(e)\n",
    "\n",
    "plt.plot(f_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ec52b-db28-40fd-8069-6f55268109f0",
   "metadata": {},
   "source": [
    "#### Newton's Algorithm.\n",
    "***summary here: initial points, solutions to each point, log-linear convergence plot***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b11350-0425-4549-9ecf-e8cd6bd9a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton setup.\n",
    "x0 = np.array([0, 0]).T # Initial guess.\n",
    "x_solve_newton = [x0]\n",
    "x= x_solve_newton[0]\n",
    "a=.01\n",
    "e=10 # Initialize!\n",
    "f_newton=[objfun(x0)]\n",
    "H=np.array([[10, 12],[12,20]])\n",
    "\n",
    "# Let's use an AMIJO line search\n",
    "def amijo(x):\n",
    "    a=1 # initiate\n",
    "    while objfun(x-a*grad(x)) > objfun(x)-a*(.5)*np.matmul(grad(x),np.matmul(np.linalg.inv(H),grad(x))):\n",
    "        a=.5*a\n",
    "    return a\n",
    "    \n",
    "    \n",
    "while e > .001:\n",
    "    a=amijo(x)\n",
    "    x= x - a*grad(x)\n",
    "    f_newton.append(math.log(abs(objfun(x)-f_true),10))\n",
    "    x_solve_newton.append(x)\n",
    "    e = np.linalg.norm(grad(x))\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "plt.plot(f_newton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b9d12-dacf-4f17-b07c-b99951c2ea0a",
   "metadata": {},
   "source": [
    "### Problem 3 (10 points) \n",
    "Let $f(x)$ and $g(x)$ be two convex functions defined on the convex set $\\mathcal{X}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1462c8c0-4908-4c4b-8aa1-836d0ec6b49b",
   "metadata": {},
   "source": [
    "**(5 points) Prove that $af(x)+bg(x)$ is convex for $a>0$ and $b>0$.** \n",
    "Define $$F(\\lambda x_1+(1-\\lambda)\\ x_2\\ )\\le\\ \\lambda F(x_1\\ )+(1-\\lambda)F(x_2\\ )$$\n",
    "Thus the inequaltiy that *must be true* if $F(x)$ is convex is $$F(\\lambda x_1+(1-\\lambda)\\ x_2\\ )\\le\\ \\lambda F(x_1\\ )+(1-\\lambda)F(x_2\\ )$$\n",
    "Now, expanding yields\n",
    "$$af\\left(\\lambda x_1+\\left(1-\\lambda\\right)x_2\\right)+bg\\left(\\lambda x_1+\\left(1-\\lambda\\right)x_2\\right)\\le\\lambda\\left(af\\left(x_1\\right)+bg\\left(x_1\\right)\\right)+\\left(1-\\lambda\\right)\\left(af\\left(x_2\\right)+bg\\left(x_2\\right)\\right)$$\n",
    "$$af\\left(\\lambda x_1+\\left(1-\\lambda\\right)x_2\\right)+bg\\left(\\lambda x_1+\\left(1-\\lambda\\right)x_2\\right)\\le a\\lambda f\\left(x_1\\right)+b\\lambda g\\left(x_1\\right)+a\\left(1-\\lambda\\right)f\\left(x_2\\right)+b\\left(1-\\lambda\\right)g\\left(x_2\\right)$$\n",
    "$$af\\left(\\lambda x_1+\\left(1-\\lambda\\right)x_2\\right)+bg\\left(\\lambda x_1+\\left(1-\\lambda\\right)x_2\\right)\\le a\\left(\\lambda f\\left(x_1\\right)+\\left(1-\\lambda\\right)f\\left(x_2\\right)\\right)+b\\left(\\lambda g\\left(x_1\\right)+\\left(1-\\lambda\\right)g\\left(x_2\\right)\\right)$$\n",
    "<br>\n",
    "Given that $f(x),g(x)$ are convex, we also have that $$f\\left(\\lambda x_1+\\left(1-\\lambda\\right)x_2\\right)\\le\\ \\lambda f\\left(x_1\\right)+\\left(1-\\lambda\\right)f\\left(x_2\\right)$$ $$g\\left(\\lambda x_1+\\left(1-\\lambda\\right)x_2\\right)\\le\\ \\lambda g\\left(x_1\\right)+\\left(1-\\lambda\\right)g\\left(x_2\\right)$$\n",
    "<br> Thus, the original inequality holds true if $a\\ge0, b\\ge0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e02e0-7907-43ad-84d6-8f600e204e7c",
   "metadata": {},
   "source": [
    "**(5 points) In what conditions will $f(g(x))$ be convex?**\n",
    "In other words, under which conditions will the following be true: $$f(g(λx_1+(1-λ) x_2 ))≤λf(g(x_1 ))+(1-λ)f(g(x_2))$$\n",
    "If $g(x)$ is convex, we know that $$g(λx_1+(1-λ) x_2 )≤ λg(x_1 )+(1-λ)g(x_2 )$$ and, we can say that $$f(g(λx_1+(1-λ) x_2 ))≤f(λg(x_1 )+(1-λ)g(x_2 ))$$ *only if $f(x)$ is* ***nondecreasing*** (because a larger argument in $f(x)$ will yield a greater-than-or-equal-to output). <br>This allows us to apply the convexity of $f(x)$ and claim that $$f(λg(x_1 )+(1-λ)g(x_2 ))≤ λf(g(x_1 ))+(1-λ)f(g(x_2 ))$$\n",
    "When we combine the previous inequalities, we yield $$f(g(λx_1+(1-λ) x_2 ))≤f(λg(x_1 )+(1-λ)g(x_2 ))≤ λf(g(x_1 ))+(1-λ)f(g(x_2 ))$$ or, $$f(g(λx_1+(1-λ) x_2 ))≤ λf(g(x_1 ))+(1-λ)f(g(x_2 ))$$ <br> Hence, $f(g(x))$ will be convex when $f(x)$ is **nondecreasing**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765136d3-020e-4b8b-bd90-c5109cd95c98",
   "metadata": {},
   "source": [
    "### Problem 4 (bonus 10 points)\n",
    "Show that $f({\\bf x}_1) \\geq f(\\textbf{x}_0) + \n",
    "    \\textbf{g}_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0)$ for a convex function $f(\\textbf{x}): \\mathcal{X} \\rightarrow \\mathbb{R}$ and for $\\textbf{x}_0$, $\\textbf{x}_1 \\in \\mathcal{X}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ad426-903c-4fee-b3c5-626e89f78b0d",
   "metadata": {},
   "source": [
    "# Design Problems\n",
    "\n",
    "### Problem 5 (20 points) \n",
    "Consider an illumination problem: There are $n$ lamps and $m$ mirrors fixed to the ground. The target reflection intensity level is $I_t$. The actual reflection intensity level on the $k$th mirror can be computed as $\\textbf{a}_k^T \\textbf{p}$, where $\\textbf{a}_k$ is given by the distances between all lamps to the mirror, and $\\textbf{p}:=[p_1,...,p_n]^T$ are the power output of the lamps. The objective is to keep the actual intensity levels as close to the target as possible by tuning the power output $\\textbf{p}$. <br> So $a_k$ is a $n * 1$ vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7925218-76be-41ef-a2da-81db3c0550aa",
   "metadata": {},
   "source": [
    "#### A. \n",
    "**(5 points) Formulate this problem as an optimization problem.<br>**\n",
    "The intensity of the reflection can be expressed as $$I_k=\\textbf{a}_{k}^T\\textbf{p}$$ and the corresponding objective function is $$\\min_{\\textbf{p}} \\sum_{k=1}^n(I_k-I_t)^2$$\n",
    "This is under the physical constraint of maximum power $p_{max}$ $$0\\le p_j \\le p_{max}\\quad \\forall j=1...m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d071f99-dd55-45d5-acf6-1fb5af26847a",
   "metadata": {},
   "source": [
    "#### B.\n",
    "**(5 points) Is your problem convex?<br>**\n",
    "The *objective function* is comprised of a summation of a quadratic operator on the linear function $I_k-I_t = \\textbf{a}_k^T\\textbf{p}-I_t$. Therefore, since the summation, quadratic operator, and linear function are all convex, the resulting objective function is also convex. <br>\n",
    "\n",
    "The objective function is $$\\sum_{k=1}^n(a_k^T p-I_t )^2 =\\sum_{k=1}^n(a_k^T p)^2-2I_t a_k^T p+I_t^2 $$ We use the Hessian to determine if it is convex (PSD Hessian). $$H_k=\\frac{\\partial^2}{∂p^2} ((a_k^T p)^2-2I_t a_k^T p+I_t^2 )=2a_k a_k^T∈\\mathbb{R}^{n×n}$$ \n",
    "Hence $$H=\\sum H_k$$ Now, we left and right multiply by a unit vector $d\\in \\mathbb{R}^{n}$ and yield $$d^THd=\\sum2d^Ta_ka_k^Td$$\n",
    "If we say $d^Ta_k=a_k^Td=b_k$, we can write $$d^THd=\\sum2b_k^2\\geq0$$  Note that $ d^THd\\geq\\lambda_{min}$ where $\\lambda_{min}$ is the smallest eigenvalue of $H$. Thus, since $\\sum2b_k^2\\geq0$, $H$ is positive semidefinite. \n",
    "\n",
    "The *constraint* on power is the intersection of half-spaces that create a hypercube, which in turn is convex. Hence the problem is convex.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7bc5e5-49cf-4891-bf5c-f10b5370dda5",
   "metadata": {},
   "source": [
    "#### C.\n",
    "**(5 points) If we require the overall power output of any of the $n$ lamps to be less than $p^*$, will the problem have a unique solution?**<br>\n",
    "The new constraint that $\\sum_i p_i<p^*$ is a linear (and thus convex) constraint. If the objective function is *strictly convex*, this will have a unique solution.\n",
    "\n",
    "The condition $\\sum2b_k^2>0 $ for $H$ to be positive definite indicates that $ \\left[\\begin{matrix}a_1^T\\\\\\vdots\\\\a_m^T\\\\\\end{matrix}\\right]\\in\\mathbb{R}^{m\\times n}$ must have a trivial null space, such that no $d$ causes $a_k^Td\\ \\forall\\ k=1\\ldots m  $ to go to zero. In other words, assuming that $a_k$ are linearly independent, there must be more mirrors $m$ than lamps $n$ for $H$ to be positive definite, the objective function to be strictly convex, and the problem to have a unique solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a000b87-d923-42fa-ab3c-73f8ad3b122c",
   "metadata": {},
   "source": [
    "#### D.\n",
    "**(5 points) If we require no more than half of the lamps to be switched on, will the problem have a unique solution?** <br> No, it will not, as the solution space is made of several intersecting line segments, and is no longer convex. The problem is now np-hard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-twins",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "For this homework, you may want to attach sketches as means to explain your ideas. Here is how you can attach images.\n",
    "\n",
    "![batman](batman.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
